{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP intro to Natural Language Procession\n",
    "\n",
    "Based on Laurence Moroney's tutorials\n",
    "\n",
    "## Contents\n",
    "0. Install packages\n",
    "1. A basic example of encoding and decoding / aka creating embeddings\n",
    "2. Tokenization\n",
    "3. Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp310-cp310-macosx_10_15_x86_64.whl (230.1 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: packaging in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: setuptools in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (4.22.1)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.8-py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.54.0.tar.gz (23.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.10.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.1)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: grpcio\n",
      "  Building wheel for grpcio (setup.py) ... \u001b[?25l|^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A basic example of encoding and decoding / aka creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dummy text\n",
    "text = 'the quick brown fox jumps over the lazy dog!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#create a set (each character one time) convert it to a list and sort it\n",
    "print(sorted(list(set(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      " !abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "print(''.join(chars)) #join the items in item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 13, 13, 16, 0, 24, 16, 19, 13, 5, 1]\n",
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = str_to_int = { ch:i for i,ch in enumerate(chars) } #stoi = string to integer\n",
    "itos = int_to_str = { i:ch for i,ch in enumerate(chars) } #itos = integer to string\n",
    "\n",
    "# encoder: take a string, output a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "# decoder: take a list of integers, output a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "print(encode(\"hello world!\"))\n",
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h e l l o\n"
     ]
    }
   ],
   "source": [
    "print(chars[9],chars[6],chars[13], chars[13], chars[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape is: torch.Size([44])\n",
      "the dtype is: torch.int64\n",
      "the tensor is: \n",
      "tensor([21,  9,  6,  0, 18, 22, 10,  4, 12,  0,  3, 19, 16, 24, 15,  0,  7, 16,\n",
      "        25,  0, 11, 22, 14, 17, 20,  0, 16, 23,  6, 19,  0, 21,  9,  6,  0, 13,\n",
      "         2, 27, 26,  0,  5, 16,  8,  1])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "# we use PyTorch: https://pytorch.org\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'the shape is: {data.shape}')\n",
    "print(f'the dtype is: {data.dtype}')\n",
    "print(f'the tensor is: \\n{data}') # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Creating a word-index with Tensorflow's Tokenizer\n",
    "#source: source: https://www.youtube.com/watch?v=fNxaJsNG3-s\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words =100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc)\n",
    "print(type(enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequencing - Turning sentences into data (NLP Zero to Hero - Part 2)\n",
    "\n",
    "https://www.youtube.com/watch?v=r9QjkdSJZ2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 5  3  2  4  0  0  0]\n",
      " [ 5  3  2  7  0  0  0]\n",
      " [ 6  3  2  4  0  0  0]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words =100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NLP @ Picnic (just notes)\n",
    "\n",
    "definition: NLP enables machines to understand human language\n",
    "\n",
    "#### Use cases\n",
    "- Chatbots\n",
    "- Recommending answers to call center agents eg based on sentiment\n",
    "- Identifying trends in customer feedback\n",
    "- Prioritization of tickets\n",
    "- Routing tickets\n",
    "\n",
    "Messages could be: positive feedback from happy customers, request to adjust delivery time, missing products, issues with rproducts eg freshness, payment questions, suggestion to products\n",
    "\n",
    "Automation with NLP.\n",
    "Picnic uses two models:\n",
    "- positive classification model =>assign positivity score => then business rules (eg no question mark, no address etc_ => then close message automatically. Circa 20% of messages are positive\n",
    "- issue classification model => freshness, completeness, payment, adjustment, assortment suggestion\n",
    "\n",
    "Text classification:\n",
    "Step 1: Text cleaning\n",
    "    a. Text substitution: example thursday => $weekday, ? => $QUESTION\n",
    "    b. Replace emojis with words\n",
    "    c. Clean punctuation\n",
    "    d. Tokenization\n",
    "    e. Stemming\n",
    "    f. Remove stop words (eg. are, and, the, etc)\n",
    "Step 2: Feature cration\n",
    "    a. Text as a feature: bag of words, N-grams, TF_IDF term ferquency-inverse document frequency, BERT (embeddings)\n",
    "    \n",
    "    TF-IDF  numbers respresenting how relevant each word is in that document. Overweighr: rare words, underweight: often used words. sklearn has Class TFidfVectorizer\n",
    "    BERT: can do Sentiment analysis, question answering text prediction etc. + get the essence of a text. BERT helps google to understand queries better. \n",
    "    Two submodels:\n",
    "        1. Masked Language Model (MLM) => bidirectional training, 15% of words is masked during training, use words on either siee of the masked word to predict them. Learns context & doesn't require labeled data.    \n",
    "        2. Next sentence prediction. (NSP) Binary classification task. Learn about relationships between sentences. \n",
    "\n",
    "There is a combined loss function of NSP & MLM. \n",
    "\n",
    "BERTje = Dutch BERT model developed at University of Groningen. \n",
    "\n",
    "Huggingface provides really nice API's:\n",
    "from transformers import BertTokeniser, BertforSequenceClassification (oid)\n",
    "\n",
    "TF-IDF works better when other features are present computationally less expensive. BERT is better for unstructured text.\n",
    "\n",
    "Step 3: Classification\n",
    "\n",
    "- logistic regression model\n",
    "    positive score, probabilty of each issue type.\n",
    "    \n",
    "\n",
    "Architecture overview: \n",
    "- Salesforce as system of record\n",
    "- Python job to request all messages (first batch job, now an API)\n",
    "- then 3 AI steps\n",
    "- send back the classifications to sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
